{
  "nbformat": 4,
  "nbformat_minor": 0, 
  "metadata": {
    "colab": {
      "name": "BERT-NER.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPO+H+SjgXa5AY3x8GE0wjS"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgUtHIwYOXZQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "a4d4d58f-8831-4d1a-9936-b524fc5962c0"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from google.colab import drive\n",
        "import pprint  # for pretty printing our device stats\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm, trange\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "#install transformers to google colab\n",
        "!pip install transformers\n",
        "from transformers import BertTokenizer, BertConfig\n",
        "import math\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import transformers\n",
        "from transformers import BertForTokenClassification, AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        " \n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "import seaborn as sns\n",
        "drive.mount('/content/gdrive')\n",
        "%cd \"/content/gdrive/My Drive/DataSets/Annotated Corpus 2\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "/content/gdrive/My Drive/DataSets/Annotated Corpus 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ipb2Fb9pFA3Y",
        "colab_type": "text"
      },
      "source": [
        "Class to simplfy Named Entity Recognition with BERT and handling tokenizing, attention mask setting, classifying, setting device and testing.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTFVOIMIreYy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Input: pandas: 0th column: text (splitted to words) ([\"i\",\"am\",\"him\"]),  1th column: labels [\"v\",\"b\",\"c\"]  \n",
        "\n",
        "Kullanım: \n",
        "NERModel = BERT4NER()\n",
        "NERModel.workOn(df)\n",
        "NERModel.initCLSF()\n",
        "NERMODEL.setFullInput() \n",
        "NERMODEL.train(epochs=3)\n",
        "\"\"\"\n",
        "\n",
        "class Bert4NER():\n",
        "\n",
        "  def __init__(self):\n",
        "      self.DataMapped = False\n",
        "      self.tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n",
        "      self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "      # device sayısı \n",
        "\n",
        "      \n",
        "  def workOn(self,data):\n",
        "    self.sentences = data.iloc[:,0]\n",
        "    self.labels = data.iloc[:,1] \n",
        "    self.tag_values = list(set(np.concatenate(self.labels.values,axis=0)))\n",
        "    # Ayrıca ek olarak PAD ekledi, paddingdeki kelmelerin ayrı sayılması için \n",
        "    self.tag_values.append(\"PAD\") \n",
        "    self.tag2idx = {t: i for i, t in enumerate(self.tag_values)} \n",
        "    self.DataMapped = True \n",
        "    print(\"Labels Mapped\") \n",
        "    \n",
        "\n",
        "  def initCLSF(self):\n",
        "        assert (self.DataMapped==True)\n",
        "        \"\"\" Bert Token verilerine özel üretilmiş bir classification modeli \"\"\"\n",
        "        self.model = BertForTokenClassification.from_pretrained(\n",
        "            \"bert-base-cased\",\n",
        "            #Labeller burada veriliyor, çıktı katmanı da ona göre hesaplanıyor sanırım\n",
        "            num_labels=len(self.tag2idx),\n",
        "            output_attentions = False,  \n",
        "            output_hidden_states = False \n",
        "        )\n",
        "        if (torch.cuda.is_available()):\n",
        "          self.model.cuda(); \"\"\" Modeln GPU kullanması için\"\"\"\n",
        "\n",
        "  def tokenize_and_preserve_labels(self,sentence, text_labels):\n",
        "    tokenized_sentence = []\n",
        "    labels = []\n",
        "\n",
        "    for word, label in zip(sentence, text_labels):\n",
        "\n",
        "        # Tokenize the word and count # of subwords the word is broken into\n",
        "        tokenized_word = self.tokenizer.tokenize(word) \n",
        "        n_subwords = len(tokenized_word)\n",
        "\n",
        "        # Add the tokenized word to the final tokenized word list\n",
        "        tokenized_sentence.extend(tokenized_word)\n",
        "\n",
        "        # Add the same label to the new list of labels `n_subwords` times\n",
        "        labels.extend([label] * n_subwords)\n",
        "\n",
        "    return tokenized_sentence, labels\n",
        "\n",
        "  #Input olarak cümleler veriliyor\n",
        "  def setPredictInput(self,test_sentence): \n",
        "\n",
        "    tokenized_sentence = self.tokenizer.encode(test_sentence)\n",
        "    input_ids = torch.tensor([tokenized_sentence]).cuda()\n",
        "    with torch.no_grad():\n",
        "      output = self.model(input_ids)\n",
        "    label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n",
        "    # join bpe split tokens\n",
        "    tokens = self.tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
        "    new_tokens, new_labels = [], []\n",
        "    for token, label_idx in zip(tokens, label_indices[0]):\n",
        "        if token.startswith(\"##\"):\n",
        "            new_tokens[-1] = new_tokens[-1] + token[2:]\n",
        "        else:\n",
        "            new_labels.append(self.tag_values[label_idx])\n",
        "            new_tokens.append(token)\n",
        "    for token, label in zip(new_tokens, new_labels):\n",
        "        print(\"{}\\t{}\".format(label, token))\n",
        "\n",
        "    return zip(new_tokens,new_labels)    \n",
        "\n",
        "\n",
        "  \"\"\" Tokenize, train-test split, padding, tensorin Creating DataLoader\"\"\"\n",
        "  def setFullInput(self,testSize= 0.1, MAX_LEN=75,batch_size=32):\n",
        "    tokenized_texts_and_labels = [\n",
        "      self.tokenize_and_preserve_labels(sent, labs) for sent, labs in zip(self.sentences, self.labels) # Tokenize edilmiş Cümle-Label çiftleri\n",
        "    ]\n",
        "\n",
        "    tokenized_texts = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels]\n",
        "    labels = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels] \n",
        "\n",
        "    print(\"Tokenization Done\")\n",
        "\n",
        "    input_ids = pad_sequences([self.tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
        "                          truncating=\"post\", padding=\"post\")\n",
        "    tags = pad_sequences([[self.tag2idx.get(l) for l in lab] for lab in labels],\n",
        "                     maxlen=MAX_LEN, value=self.tag2idx[\"PAD\"], padding=\"post\",\n",
        "                     dtype=\"long\", truncating=\"post\")\n",
        "    \n",
        "    print(\"Padding is Done\")\n",
        "\n",
        "    attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]\n",
        "    tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags,\n",
        "                                                                random_state=2018, test_size=testSize)\n",
        "    tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "                                                random_state=2018, test_size=testSize)\n",
        "    \n",
        "\n",
        "    \"\"\" tensor'a dönüşen veriler sonradan gpu'ya ya da cpu'ya konulma gibi özelliklere sahip olabiliyorlar \"\"\"\n",
        "    tr_inputs = torch.tensor(tr_inputs)  \n",
        "    val_inputs = torch.tensor(val_inputs)\n",
        "    tr_tags = torch.tensor(tr_tags)\n",
        "    val_tags = torch.tensor(val_tags)\n",
        "    tr_masks = torch.tensor(tr_masks)\n",
        "    val_masks = torch.tensor(val_masks)\n",
        "\n",
        "    print(\"Tensoring is Done\")\n",
        "\n",
        "    train_data = TensorDataset(tr_inputs, tr_masks, tr_tags) \n",
        "    train_sampler = RandomSampler(train_data)\n",
        "\n",
        "    \"\"\" train_data ve train_sampler birleştirilerek train_dataloader oluşturuluyor.\"\"\"\n",
        "    self.train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size) \n",
        "    del train_data\n",
        "    del train_sampler\n",
        "\n",
        "    valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
        "    valid_sampler = SequentialSampler(valid_data)\n",
        "\n",
        "    \"\"\" Aynısı validation data için de geçerli\"\"\"\n",
        "\n",
        "    self.valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=batch_size)\n",
        "    del valid_data\n",
        "    del valid_sampler\n",
        "    print(\"DataLoaders are Ready\")\n",
        "    \n",
        "\n",
        "  def initOptimizer(self,FULL_FINETUNING=False,epochs=1): \n",
        "\n",
        "    if FULL_FINETUNING:\n",
        "        param_optimizer = list(self.model.named_parameters())  #Çok uzun bir liste, büyük ihtimalle modelin ağırlıkları ile ilgili değerler \n",
        "        \"\"\" param_optimizer, (201,2) shape'inde bir liste \"\"\"\n",
        "\n",
        "\n",
        "        no_decay = ['bias', 'gamma', 'beta']\n",
        "\n",
        "        \"\"\" optimizer_grouped_parameters, 2 key'e sahip 2 tane dictionary'den oluşan bir liste. \n",
        "            dictionary'nin params anahtarının açılaması şöyle:\n",
        "\n",
        "            param_optimizer: \n",
        "            n p\n",
        "            n p\n",
        "            n p\n",
        "            n p\n",
        "            .\n",
        "            .\n",
        "            .\n",
        "            (201 tane)\n",
        "            \n",
        "            buradaki p'leri seçiyor EĞER  n'in içinde hiç 'bias', 'gamma' ya da 'beta' yoksa\n",
        "            \"\"\"\n",
        "        optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "            'weight_decay_rate': 0.01},\n",
        "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "            'weight_decay_rate': 0.0}\n",
        "        ] \n",
        "    else:\n",
        "\n",
        "        \"\"\" Eğer full finetuning yoksa, fine_tuning'i sadece model.classifier'ın parametreleri için yapıyor\"\"\"\n",
        "        param_optimizer = list(self.model.classifier.named_parameters())\n",
        "        optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
        "\n",
        "    \"\"\" AdamW'nin tanımı: Implements Adam algorithm with weight decay fix \n",
        "      Kısacası, parametreleri vererek o parametrelere göre bir optimizer oluşturuyor.\n",
        "      AdamW ile ilgili makale: https://arxiv.org/abs/1711.05101\n",
        "\n",
        "    \"\"\"\n",
        "    self.optimizer = AdamW(\n",
        "        optimizer_grouped_parameters,\n",
        "        lr=3e-5,\n",
        "        eps=1e-8\n",
        "    ) \n",
        "\n",
        "\n",
        "    # Total number of training steps is number of batches * number of epochs.\n",
        "    total_steps = len(self.train_dataloader) * epochs\n",
        "\n",
        "    # Create the learning rate Decay.\n",
        "    self.lrDecay = get_linear_schedule_with_warmup(\n",
        "        self.optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "    print(\"Optimizer Ready\\n\")\n",
        "\n",
        "\n",
        "  def test(self):\n",
        "    # Put the model into evaluation mode\n",
        "        self.model.eval()\n",
        "        # Reset the validation loss for this epoch.\n",
        "        eval_loss, eval_accuracy = 0, 0\n",
        "        nb_eval_steps, nb_eval_examples = 0, 0\n",
        "        predictions , true_labels = [], []\n",
        "        for batch in self.valid_dataloader:\n",
        "    \n",
        "            batch = tuple(t.to(self.device) for t in batch)\n",
        "\n",
        "\n",
        "            b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "            # Telling the model not to compute or store gradients,\n",
        "            # saving memory and speeding up validation\n",
        "            with torch.no_grad():\n",
        "                # Forward pass, calculate logit predictions.\n",
        "                # This will return the logits rather than the loss because we have not provided labels.\n",
        "                outputs = self.model(b_input_ids, token_type_ids=None,\n",
        "                                attention_mask=b_input_mask, labels=b_labels)\n",
        "            # Move logits and labels to CPU\n",
        "            logits = outputs[1].detach().cpu().numpy()\n",
        "            label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "            # Calculate the accuracy for this batch of test sentences.\n",
        "            eval_loss += outputs[0].mean().item()\n",
        "            predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
        "            true_labels.extend(label_ids)\n",
        "\n",
        "        eval_loss = eval_loss / len(self.valid_dataloader)\n",
        "        validation_loss_values.append(eval_loss)\n",
        "        print(\"Validation loss: {}\".format(eval_loss))\n",
        "        pred_tags = [self.tag_values[p_i] for p, l in zip(predictions, true_labels)\n",
        "                                    for p_i, l_i in zip(p, l) if self.tag_values[l_i] != \"PAD\"]\n",
        "        valid_tags = [self.tag_values[l_i] for l in true_labels\n",
        "                                      for l_i in l if self.tag_values[l_i] != \"PAD\"]\n",
        "        print(\"Validation Accuracy: {}\".format(accuracy_score(pred_tags, valid_tags)))  \n",
        "        print(\"Validation F1 Score: {}\".format(f1_score(pred_tags, valid_tags,average=\"weighted\")))\n",
        "  def train(self,epochs=1,max_grad_norm=1.0,initOpt = True):\n",
        "    if(initOpt):\n",
        "      self.initOptimizer(epochs=epochs)\n",
        "    ## Store the average loss after each epoch so we can plot them.\n",
        "    loss_values, validation_loss_values = [], []\n",
        "\n",
        "    for _ in trange(epochs, desc=\"Epoch\"): \n",
        "\n",
        "        # Put the model into training mode.\n",
        "        self.model.train()\n",
        "        # Reset the total loss for this epoch.\n",
        "        total_loss = 0\n",
        "\n",
        "        # Training loop\n",
        "\n",
        "         \"\"\" Önceden tensor dataset ve random sampler ile birleştirerek elde ettiğimiz data_loader'ı enumerate ederek step ve batch'i çekiyoruz\"\"\"\n",
        "        for step, batch in enumerate(self.train_dataloader):\n",
        "            # add batch to gpu\n",
        "            batch = tuple(t.to(self.device) for t in batch)\n",
        "            b_input_ids, b_input_mask, b_labels = batch\n",
        "            # Always clear any previously calculated gradients before performing a backward pass.\n",
        "            self.model.zero_grad()\n",
        "            # forward pass\n",
        "            # This will return the loss (rather than the model output)\n",
        "            # because we have provided the `labels`. \n",
        "            outputs = self.model(b_input_ids, token_type_ids=None,\n",
        "                            attention_mask=b_input_mask, labels=b_labels)\n",
        "            # get the loss\n",
        "            loss = outputs[0]\n",
        "            # Perform a backward pass to calculate the gradients.\n",
        "            loss.backward()\n",
        "            # track train loss\n",
        "            total_loss += loss.item()\n",
        "            # Clip the norm of the gradient\n",
        "            # This is to help prevent the \"exploding gradients\" problem.\n",
        "            \n",
        "            \"\"\" Gradyan vektörlerin normlarını yani orijine olan uzaklıklarını kırpıyor: yüksek olanları beliri bir seviyeye indiriyor \n",
        "                Böylece yüksek sayılara ulaşması önlenmiş oluyor\n",
        "             \"\"\"\n",
        "            torch.nn.utils.clip_grad_norm_(parameters=self.model.parameters(), max_norm=max_grad_norm)\n",
        "\n",
        "            \n",
        "            # update parameters\n",
        "            self.optimizer.step()\n",
        "            # Update the learning rate.\n",
        "            self.lrDecay.step()\n",
        "\n",
        "        # Calculate the average loss over the training data.\n",
        "        avg_train_loss = total_loss / len(self.train_dataloader)\n",
        "        print(\"Average train loss: {}\".format(avg_train_loss))\n",
        "\n",
        "        # Store the loss value for plotting the learning curve.\n",
        "        loss_values.append(avg_train_loss)\n",
        " \n",
        "        # Put the model into evaluation mode\n",
        "        self.model.eval()\n",
        "        # Reset the validation loss for this epoch.\n",
        "        eval_loss, eval_accuracy = 0, 0\n",
        "        nb_eval_steps, nb_eval_examples = 0, 0\n",
        "        predictions , true_labels = [], []\n",
        "        for batch in self.valid_dataloader:\n",
        "    \n",
        "            batch = tuple(t.to(self.device) for t in batch)\n",
        "\n",
        "\n",
        "            b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "            # Telling the model not to compute or store gradients,\n",
        "            # saving memory and speeding up validation\n",
        "            with torch.no_grad():\n",
        "                # Forward pass, calculate logit predictions.\n",
        "                # This will return the logits rather than the loss because we have not provided labels.\n",
        "                outputs = self.model(b_input_ids, token_type_ids=None,\n",
        "                                attention_mask=b_input_mask, labels=b_labels)\n",
        "            # Move logits and labels to CPU\n",
        "            logits = outputs[1].detach().cpu().numpy()\n",
        "            label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "            # Calculate the accuracy for this batch of test sentences.\n",
        "            eval_loss += outputs[0].mean().item()\n",
        "            predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
        "            true_labels.extend(label_ids)\n",
        "\n",
        "        eval_loss = eval_loss / len(self.valid_dataloader)\n",
        "        validation_loss_values.append(eval_loss)\n",
        "        print(\"Validation loss: {}\".format(eval_loss))\n",
        "        pred_tags = [self.tag_values[p_i] for p, l in zip(predictions, true_labels)\n",
        "                                    for p_i, l_i in zip(p, l) if self.tag_values[l_i] != \"PAD\"]\n",
        "        valid_tags = [self.tag_values[l_i] for l in true_labels\n",
        "                                      for l_i in l if self.tag_values[l_i] != \"PAD\"]\n",
        "        print(\"Validation Accuracy: {}\".format(accuracy_score(pred_tags, valid_tags)))  \n",
        "        print(\"Validation F1 Score: {}\".format(f1_score(pred_tags, valid_tags,average=\"weighted\")))\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69h6vhDU2BLi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "ca46c592-3545-40d2-af71-e75cb4d6ab86"
      },
      "source": [
        "data = pd.read_csv(\"ner_datasetreference.csv\", encoding=\"latin1\").fillna(method=\"ffill\")\n",
        "data.tail(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence #</th>\n",
              "      <th>Word</th>\n",
              "      <th>POS</th>\n",
              "      <th>Tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1048565</th>\n",
              "      <td>Sentence: 47958</td>\n",
              "      <td>impact</td>\n",
              "      <td>NN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1048566</th>\n",
              "      <td>Sentence: 47958</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1048567</th>\n",
              "      <td>Sentence: 47959</td>\n",
              "      <td>Indian</td>\n",
              "      <td>JJ</td>\n",
              "      <td>B-gpe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1048568</th>\n",
              "      <td>Sentence: 47959</td>\n",
              "      <td>forces</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1048569</th>\n",
              "      <td>Sentence: 47959</td>\n",
              "      <td>said</td>\n",
              "      <td>VBD</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1048570</th>\n",
              "      <td>Sentence: 47959</td>\n",
              "      <td>they</td>\n",
              "      <td>PRP</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1048571</th>\n",
              "      <td>Sentence: 47959</td>\n",
              "      <td>responded</td>\n",
              "      <td>VBD</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1048572</th>\n",
              "      <td>Sentence: 47959</td>\n",
              "      <td>to</td>\n",
              "      <td>TO</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1048573</th>\n",
              "      <td>Sentence: 47959</td>\n",
              "      <td>the</td>\n",
              "      <td>DT</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1048574</th>\n",
              "      <td>Sentence: 47959</td>\n",
              "      <td>attack</td>\n",
              "      <td>NN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Sentence #       Word  POS    Tag\n",
              "1048565  Sentence: 47958     impact   NN      O\n",
              "1048566  Sentence: 47958          .    .      O\n",
              "1048567  Sentence: 47959     Indian   JJ  B-gpe\n",
              "1048568  Sentence: 47959     forces  NNS      O\n",
              "1048569  Sentence: 47959       said  VBD      O\n",
              "1048570  Sentence: 47959       they  PRP      O\n",
              "1048571  Sentence: 47959  responded  VBD      O\n",
              "1048572  Sentence: 47959         to   TO      O\n",
              "1048573  Sentence: 47959        the   DT      O\n",
              "1048574  Sentence: 47959     attack   NN      O"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqnnva5tEk0-",
        "colab_type": "text"
      },
      "source": [
        "Preprcoessing on data to match input format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpIdgWEd22xO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "d5a1892b-ad66-47e4-9c65-34b3218dfa7a"
      },
      "source": [
        "group = data.groupby(\"Sentence #\")\n",
        "preprocessed = data.drop(columns=[\"POS\"])\n",
        "preprocessed = pd.concat([group[\"Word\"].apply(list).reset_index(name=\"Word\")[\"Word\"], \n",
        "          group[\"Tag\"].apply(list).reset_index(name=\"Tag\")[\"Tag\"]],axis=1)\n",
        "allSentencesIdx = [] \n",
        "for i in range(len(preprocessed)):\n",
        "  if (len(set(preprocessed.iloc[i][\"Tag\"]))==1 and preprocessed.iloc[i][\"Tag\"][0] == \"O\"):\n",
        "    allSentencesIdx.append(preprocessed.index[i])\n",
        "\n",
        "preprocessed = preprocessed.drop(allSentencesIdx)    \n",
        "\n",
        "for i in range(len(preprocessed)):\n",
        "  assert (len(set(preprocessed.iloc[i][\"Tag\"]))!=1) \n",
        "\n",
        "preprocessed.head(10)\n",
        "preprocessed"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Word</th>\n",
              "      <th>Tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[Thousands, of, demonstrators, have, marched, ...</td>\n",
              "      <td>[O, O, O, O, O, O, B-geo, O, O, O, O, O, B-geo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[Iranian, officials, say, they, expect, to, ge...</td>\n",
              "      <td>[B-gpe, O, O, O, O, O, O, O, O, O, O, O, O, O,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[Helicopter, gunships, Saturday, pounded, mili...</td>\n",
              "      <td>[O, O, B-tim, O, O, O, O, O, B-geo, O, O, O, O...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[U.N., relief, coordinator, Jan, Egeland, said...</td>\n",
              "      <td>[B-geo, O, O, B-per, I-per, O, B-tim, O, B-geo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[Mr., Egeland, said, the, latest, figures, sho...</td>\n",
              "      <td>[B-per, I-per, O, O, O, O, O, O, O, O, O, O, O...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47953</th>\n",
              "      <td>[Opposition, activists, have, called, for, pro...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, B-tim, I-tim, O, O...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47954</th>\n",
              "      <td>[Opposition, leader, Mir, Hossein, Mousavi, ha...</td>\n",
              "      <td>[O, O, O, B-per, I-per, O, O, O, O, O, O, O, O...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47955</th>\n",
              "      <td>[On, Thursday, ,, Iranian, state, media, publi...</td>\n",
              "      <td>[O, B-tim, O, B-gpe, O, O, O, O, O, O, O, O, B...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47956</th>\n",
              "      <td>[Following, Iran, 's, disputed, June, 12, elec...</td>\n",
              "      <td>[O, B-geo, O, O, B-tim, I-tim, O, O, O, O, O, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47958</th>\n",
              "      <td>[The, United, Nations, is, praising, the, use,...</td>\n",
              "      <td>[O, B-org, I-org, O, O, O, O, O, O, O, O, O, O...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>40917 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    Word                                                Tag\n",
              "0      [Thousands, of, demonstrators, have, marched, ...  [O, O, O, O, O, O, B-geo, O, O, O, O, O, B-geo...\n",
              "1      [Iranian, officials, say, they, expect, to, ge...  [B-gpe, O, O, O, O, O, O, O, O, O, O, O, O, O,...\n",
              "2      [Helicopter, gunships, Saturday, pounded, mili...  [O, O, B-tim, O, O, O, O, O, B-geo, O, O, O, O...\n",
              "4      [U.N., relief, coordinator, Jan, Egeland, said...  [B-geo, O, O, B-per, I-per, O, B-tim, O, B-geo...\n",
              "5      [Mr., Egeland, said, the, latest, figures, sho...  [B-per, I-per, O, O, O, O, O, O, O, O, O, O, O...\n",
              "...                                                  ...                                                ...\n",
              "47953  [Opposition, activists, have, called, for, pro...  [O, O, O, O, O, O, O, O, O, B-tim, I-tim, O, O...\n",
              "47954  [Opposition, leader, Mir, Hossein, Mousavi, ha...  [O, O, O, B-per, I-per, O, O, O, O, O, O, O, O...\n",
              "47955  [On, Thursday, ,, Iranian, state, media, publi...  [O, B-tim, O, B-gpe, O, O, O, O, O, O, O, O, B...\n",
              "47956  [Following, Iran, 's, disputed, June, 12, elec...  [O, B-geo, O, O, B-tim, I-tim, O, O, O, O, O, ...\n",
              "47958  [The, United, Nations, is, praising, the, use,...  [O, B-org, I-org, O, O, O, O, O, O, O, O, O, O...\n",
              "\n",
              "[40917 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrkjpXfcEo5H",
        "colab_type": "text"
      },
      "source": [
        "Create model and work on preprocessed data, initialize classifier and set full input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSxvU8m93B-Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "outputId": "c863ae43-0427-4cf6-8499-84ceac729c06"
      },
      "source": [
        "\n",
        "del NERModel\n",
        "NERModel = Bert4NER()\n",
        "NERModel.workOn(preprocessed)\n",
        "NERModel.initCLSF()\n",
        "NERModel.setFullInput() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Labels Mapped\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Tokenization Done\n",
            "Padding is Done\n",
            "Tensoring is Done\n",
            "DataLoaders are Ready\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5oZwJSNE79f",
        "colab_type": "text"
      },
      "source": [
        "Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrgLA2Uq3VjP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "61856067-9b44-4bdb-ead2-6a6b924bcecb"
      },
      "source": [
        "NERModel.train(epochs=3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Optimizer Ready\n",
            "\n",
            "Average train loss: 1.4441184082515959\n",
            "Validation loss: 0.8749889815226197\n",
            "Validation Accuracy: 0.77870480731029\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch:  33%|███▎      | 1/3 [14:50<29:40, 890.20s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation F1 Score: 0.8748786745922239\n",
            "Average train loss: 0.7883829250737545\n",
            "Validation loss: 0.7012383001856506\n",
            "Validation Accuracy: 0.7909933383226617\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch:  67%|██████▋   | 2/3 [29:39<14:50, 890.07s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation F1 Score: 0.8707480268284599\n",
            "Average train loss: 0.7011766467893983\n",
            "Validation loss: 0.6648929389193654\n",
            "Validation Accuracy: 0.798865389768181\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch: 100%|██████████| 3/3 [44:31<00:00, 890.51s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation F1 Score: 0.8710316494640086\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t282XEKLE-Z4",
        "colab_type": "text"
      },
      "source": [
        "Testing Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnlPRXNOOl7Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        },
        "outputId": "131c9548-86a7-4e95-f6b7-f5ee8175b91f"
      },
      "source": [
        "NERModel.setPredictInput( \"A statement from the U.S. Consumer Product Safety Commission said Tuesday the recall involves a play kitchen learning toy made in Mexico and imported by Fisher-Price , a division of Mattel .\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O\t[CLS]\n",
            "O\tA\n",
            "O\tstatement\n",
            "O\tfrom\n",
            "O\tthe\n",
            "B-geo\tU\n",
            "B-geo\t.\n",
            "B-geo\tS\n",
            "B-geo\t.\n",
            "I-org\tConsumer\n",
            "O\tProduct\n",
            "I-org\tSafety\n",
            "I-org\tCommission\n",
            "O\tsaid\n",
            "O\tTuesday\n",
            "O\tthe\n",
            "O\trecall\n",
            "O\tinvolves\n",
            "O\ta\n",
            "O\tplay\n",
            "O\tkitchen\n",
            "O\tlearning\n",
            "O\ttoy\n",
            "O\tmade\n",
            "O\tin\n",
            "O\tMexico\n",
            "O\tand\n",
            "O\timported\n",
            "O\tby\n",
            "B-per\tFisher\n",
            "O\t-\n",
            "O\tPrice\n",
            "O\t,\n",
            "O\ta\n",
            "O\tdivision\n",
            "O\tof\n",
            "B-geo\tMattel\n",
            "O\t.\n",
            "O\t[SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<zip at 0x7f7c71c98f48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 243
        }
      ]
    }
  ]
}
